(ns ai-project.core
  (:gen-class))
(use 'clojure.repl) ; for doc
(use 'criterium.core) ; benchmarking
(use 'clojure.core.matrix) ; math
(use 'clojure.math.combinatorics) ; math

(set-current-implementation :vectorz); matrix computations
(require '[clojure.java.io :as io]); io resources
(require '[incanter.core :as i]); statistics library
(require '[incanter.datasets :as ds]); datasets, get-dataset
(require '[incanter.excel :as xls]); excel
(require '[incanter.stats :as stat]); stats
(require '[incanter.charts :as chart]); charts
(require '[incanter.io :as iio]); csv

(def msong 
  "unchanged dataset" 
  (iio/read-dataset (str (io/resource "msong.csv")) :header true))

(defn l2v
  "convert list to vector matrix"
  [matrix]
  (mapv #(into [] %) matrix))

(defn gen-matrix
  "generates a `r` by `c` matrix with random weights between -1 and 1."
	[r c & m]
	(for [_ (take r (range))] 
   (for [_ (take c (range))] 
     (* (if (< 0.5 (rand)) -1 1) (rand)))))

(defn weight-gen 
  "generates a multitiered matrix"
  [lst]
  (loop [acc (transient []) t lst]
    (if (= 1 (count t))
           (persistent! acc)
           (recur (conj! acc (l2v (apply gen-matrix t))) (drop 1 t)))))

(defn fn-fold
  "max values for a matrix"
  [fn lst]
  (loop [acc (transient []) t lst]
    (if (every? empty? t)
      (rseq (persistent! acc))
      (recur (conj! acc (apply fn (map peek t))) (map pop t))))); handle scaling for negative attributes

(defn bias [lst]
  (mapv (fn [x] (mapv #(+ % (* rand 0.01) x)) lst)))

(defn scaled 
  [x mn mx]
  (/ (- x mn) (- mx mn)))

(defn norm-scale
  "scales values in matrix to a range between -1 and 1, utilizing max-fld"
  [lst]
  (let [mx (fn-fold max lst)
        mn (fn-fold min lst)]
        (mapv (fn [x] (mapv scaled x mn mx)) lst)))

(defn bias
  "biases an ANN"
  [lst]
    (mapv (fn [x] (mapv #(+ % 0.0001) x)) lst))

(defn sigmoid
  "takes in `z` and throws it in the sigmoid function\n"
  [z]
    (/ 1 (+ 1 (Math/exp (* -1 z)))))

(defn mmap
  "maps a function on a weight vector matrix"
  [function matrix]
  (mapv #(mapv function %) matrix))

(defn sigmoid-prime
  [z]
  (let [enz (Math/exp (* -1 z))]; e^(-z)])
    (/ enz (Math/pow (+ 1 enz) 2)))); enz/(1+enz)^2)

(defn pmm [m]
  (pm m {:formatter (fn [x] (format "%.6f" (double x)))}))
(defn feed-one
  "feeds data into nn and returns adjusted weights"
  [row w lr & {:keys [training] :or {training false}}]
  (let [x (pop row)
        y (peek row)
        [w1 w2] w
        z2 (dot x w1)
        a2 (mapv sigmoid z2)
        [z3] (dot a2 w2)
        yhat (sigmoid z3)]
      
  (if training 
    (let [ycost (* -1 (- y yhat)); -(y-yhat)]
          xt (transpose [x]); [[x1 x2 x3]]  to [[x1] [x2] [x3]]
          [w2t] (transpose w2)
          a2t (transpose [a2])
          sigmoid-prime-z3 (sigmoid-prime z3)
          delta-w2 (mmap #(* (* ycost sigmoid-prime-z3) %) a2t)
          lr-delta-w2 (mmap #(* % lr) delta-w2)
          new-w2 (i/minus w2 lr-delta-w2)
          sigmoid-prime-z2 (mapv sigmoid-prime z2)
          w2t-sigpz2 (mul w2t sigmoid-prime-z2)
          spzc  (* ycost sigmoid-prime-z3)
          wss (mapv #(* spzc %) w2t-sigpz2)
          delta-w1 (mapv #(let [[x] %1] (mul wss x)) xt)
          lr-delta-w1 (mmap #(* lr %) delta-w1)
          new-w1 (i/minus w1 lr-delta-w1)
          new-w [new-w1 new-w2]] new-w)
    (let [yc (abs (- y yhat))
          yc2 (if (> yc 0.12) yc 0.0)] 
      [y yhat yc]))))

(defn feed
  "loops across input and adjustes the weights for all of it. 
  `input` assumes y values are at the end of the vectors"
  [input weight learnrate & {:keys [training] :or {training false}}]
  (let [total (count input)]
    (if training
      (loop [x input w weight]
        (if (every? empty? x)
          w
          (recur (pop x) (let [thisx (peek x)
                               cnt (count x)
                               lr (* (/ cnt total) learnrate)] 
                           (feed-one thisx w lr :training training)))))
      (loop [x input er 0.0 acc []]
        (if (every? empty? x)
          [er (/ er total) acc]
          (let [row (peek x)
                lr 0.1
                [y yhat this-error] (feed-one row weight lr :training false)]
            (recur
              (pop x)
              (+ er this-error)
              (conj acc [yhat y this-error]))))))))

(defn refeed 
  "refeeds results from training at different learning rates `lrs`"
  [data weight lrs]
  (loop [w weight lr lrs]
    (if (empty? lr)
      w
      (recur (feed data w (first lr) :training true) (rest lr)))))

(defn expand
  "expands the dataset for testing"
  [dataset magnitude]
  (loop [ds dataset m (- magnitude 1)]
    (if (<= m 0)
      (shuffle (shuffle ds))
      (recur (into [] (concat ds dataset)) (- m 1) ))))

(defn nifty-feeder
  "expands and feeds a dataset, useful for finding that special rate"
  [data magnitude lrs size  & {:keys [verbose-flag] :or {verbose-flag false}}]
  (let [dt (expand data magnitude)
        w (weight-gen size)
        w2 (refeed dt w lrs)
        ]
    (when verbose-flag (println "Initial Weights") 
      (println "w1")
      (pm (first w))
      (println "w2")
      (pm (last w)))
     w2))

(def msong1 
  "reordered dataset" 
  (i/$ [ :artist_familiarity
				 :duration
				 :end_of_fade_in
				 :key
				 ; :key_confidence
				 :loudness
				 :mode
				 ; :mode_confidence
				 :start_of_fade_out
				 :tempo
				 :time_signature
				 ; :time_signature_confidence
				 :year
				 :artist_hotttnesss
				 :song_hotttnesss] msong))

(def msong2 
  "vector version, reordered dataset" 
  (i/to-vect msong1))

(def msong3
  "filtered vector dataset"
  (let [f1 (filterv (fn [row] 
                      (every? #(not= "nan" %) row)) msong2)
        f2 (filterv #(not= 0.0 (peek %)) f1)] 
    f2))

(def msongv 
  "scaled vector dataset" 
 (bias (norm-scale (bias msong3))))

(defn hn [input output samples alpha]
  (/ samples (* alpha (+ input output))))

(defn cnt [] 
  (concat (list (- (count (first msongv)) 1)) (list 70 1)))

(let [verbose false
      tsize 217
      shuffled-set (shuffle (shuffle msongv))
      test-set (into [] (take tsize shuffled-set))
      training-set (into [] (drop tsize shuffled-set))]
        (def w
          "adjusted weights for msongv with nifty-feeder"
          (nifty-feeder training-set 5 [0.5 0.1] (cnt) :verbose-flag verbose))

        (when verbose
          (println "Final Weights")
          (println "w1") (pm (first w))
          (println "w2") (pm (last w)))
            
        (def ec 
          "count errors"
          (feed test-set w 0.1 :training false))
        (let [ymean (second ec)
              ace (last ec)]
          
        (def variance (/ (reduce + 
                                 (mapv #(let [[y yhat ycost] %1]
                                          (Math/pow (- ycost ymean) 2)) ace)) tsize))
        
        (def standard-error (Math/sqrt variance))
        (def wx [0.944 0.912 0.148 0.000 0.545 0.197 0.644 1.000 0.672 0.635 0.500 0.000 0.675])
        (def ww1 [[0.903 0.211 0.309 0.679 0.816 0.638 0.404 0.421 0.124 0.288 0.685 0.759 0.252 0.042 0.930 0.046 0.767 0.925 0.218 0.902 0.614 0.645 0.581 0.052 0.686 0.803 0.893 0.863 0.048 0.634 0.546 0.138 0.036 0.276 0.360 0.479 0.613 0.197 0.872 0.312 0.829 0.616 0.620 0.498 0.953 0.423 0.270 0.310 0.877 0.400 0.502 0.426 0.692 0.269 0.347 0.509 0.879 0.445 0.281 0.910 0.482 0.157 0.761 0.963 0.704 0.024 0.034 0.407 0.844 0.025 0.996 0.452 0.156 0.267 0.591 0.136 0.231 0.891 0.753 0.659]
                  [0.740 0.969 0.160 0.579 0.953 0.254 0.153 0.862 0.236 0.219 0.989 0.022 0.898 0.884 0.257 0.772 0.093 0.568 0.662 0.904 0.187 0.843 0.596 0.384 0.955 0.653 0.865 0.962 0.188 0.770 0.702 0.986 0.827 0.490 0.733 0.440 0.536 0.782 0.855 0.693 0.682 0.858 0.115 0.857 0.090 0.523 0.518 0.140 0.545 0.480 0.368 0.096 0.961 0.208 0.194 0.208 0.328 0.936 0.507 0.803 0.041 0.704 0.657 0.921 0.967 0.500 0.831 0.371 0.686 0.981 0.799 0.887 0.626 0.343 0.359 0.993 0.462 0.492 0.559 0.905]
                  [0.022 0.791 0.678 0.555 0.115 0.297 0.648 0.635 0.151 0.588 0.046 0.352 0.177 0.003 0.061 0.443 0.935 0.338 0.857 0.909 0.871 0.833 0.577 0.804 0.604 0.591 0.809 0.384 0.210 0.391 0.742 0.900 0.821 0.244 0.368 0.318 0.476 0.935 0.036 0.284 0.847 0.207 0.391 0.749 0.460 0.237 0.215 0.737 0.286 0.524 0.989 0.911 0.149 0.330 0.037 0.706 0.185 0.397 0.985 0.061 0.753 0.721 0.855 0.942 0.529 0.793 0.570 0.157 0.240 0.882 0.791 0.227 0.415 0.579 0.996 0.408 0.782 0.081 0.472 0.309]
                  [0.446 0.383 0.400 0.939 0.789 0.879 0.541 0.427 0.116 0.618 0.126 0.092 0.413 0.854 0.425 0.063 0.241 0.354 0.123 0.090 0.715 0.360 0.081 0.789 0.702 0.993 0.271 0.098 0.878 0.408 0.679 0.339 0.043 0.925 0.743 0.910 0.137 0.966 0.473 0.294 0.722 0.614 0.734 0.780 0.345 0.252 0.297 0.998 0.020 0.809 0.175 0.150 0.316 0.840 0.447 0.595 0.924 0.804 0.255 0.271 0.369 0.637 0.881 0.300 0.502 0.964 0.875 0.978 0.735 0.054 0.000 0.304 0.558 0.937 0.360 0.120 0.719 0.938 0.450 0.920]
                  [0.607 0.251 0.035 0.403 0.804 0.072 0.684 0.868 0.448 0.243 0.522 0.642 0.081 0.325 0.260 0.812 0.873 0.501 0.836 0.685 0.690 0.749 0.081 0.544 0.306 0.266 0.644 0.502 0.151 0.214 0.068 0.425 0.387 0.493 0.279 0.211 0.783 0.368 0.977 0.977 0.808 0.487 0.353 0.016 0.591 0.012 0.335 0.756 0.925 0.344 0.074 0.345 0.289 0.203 0.722 0.866 0.923 0.602 0.611 0.669 0.381 0.634 0.867 0.767 0.595 0.811 0.099 0.753 0.444 0.131 0.282 0.237 0.988 0.967 0.147 0.369 0.035 0.318 0.407 0.631]
                  [0.479 0.991 0.925 0.693 0.871 0.768 0.487 0.807 0.046 0.698 0.201 0.913 0.206 0.218 0.659 0.344 0.784 0.950 0.907 0.789 0.553 0.644 0.963 0.951 0.937 0.934 0.088 0.178 0.166 0.284 0.258 0.168 0.996 0.748 0.340 0.768 0.944 0.912 0.260 0.763 0.635 0.755 0.070 0.537 0.663 0.928 0.179 0.435 0.534 0.376 0.186 0.677 0.700 0.095 0.673 0.715 0.351 0.271 0.059 0.997 0.659 0.972 0.093 0.188 0.414 0.110 0.861 0.063 0.551 0.367 0.248 0.415 0.878 0.357 0.621 0.557 0.452 0.384 0.154 0.181]
                  [0.413 0.770 0.595 0.638 0.788 0.864 0.489 0.961 0.696 0.559 0.808 0.290 0.664 0.302 0.739 0.713 0.237 0.387 0.852 0.909 0.891 0.542 0.244 0.396 0.334 0.658 0.300 0.651 0.624 0.834 0.622 0.824 0.192 0.786 0.322 0.278 0.778 0.054 0.296 0.755 0.478 0.388 0.364 0.505 0.615 0.155 0.092 0.326 0.866 0.577 0.973 0.123 0.186 0.178 0.134 0.218 0.340 0.824 0.132 0.373 0.510 0.376 0.510 0.191 0.303 0.051 0.681 0.346 0.681 0.270 0.422 0.849 0.773 0.921 0.696 0.409 0.283 0.712 0.810 0.781]
                  [0.193 0.496 0.909 0.896 0.798 0.256 0.656 0.445 0.653 0.720 0.515 0.595 0.899 0.423 0.446 0.430 0.555 0.412 0.971 0.760 0.804 0.658 0.517 0.974 0.999 0.709 0.639 0.662 0.084 0.051 0.264 0.867 0.433 0.149 0.348 0.936 0.600 0.154 0.409 0.114 0.356 0.847 0.256 0.224 0.472 0.430 0.838 0.995 0.092 0.440 0.577 0.867 0.241 0.180 0.694 0.390 0.937 0.751 0.047 0.650 0.633 0.993 0.728 0.511 0.214 0.361 0.207 0.858 0.592 0.054 0.113 0.407 0.003 0.534 0.863 0.144 0.479 0.857 0.193 0.880]
                  [0.607 0.182 0.318 0.960 0.613 0.152 0.905 0.684 0.013 0.812 0.216 0.560 0.550 0.194 0.833 0.147 0.910 0.649 0.111 0.828 0.769 0.600 0.113 0.578 0.019 0.371 0.087 0.429 0.674 0.390 0.685 0.107 0.805 0.158 0.056 0.050 0.069 0.022 0.582 0.818 0.817 0.984 0.549 0.715 0.515 0.489 0.255 0.318 0.232 0.351 0.120 0.453 0.240 0.803 0.147 0.121 0.352 0.903 0.708 0.793 0.492 0.416 0.984 0.460 0.112 0.617 0.545 0.876 0.522 0.405 0.516 0.425 0.878 0.499 0.464 0.147 0.578 0.492 0.669 0.545]
                  [0.065 0.001 0.707 0.922 0.596 0.202 0.343 0.197 0.369 0.567 0.578 0.295 0.482 0.734 0.316 0.470 0.640 0.032 0.533 0.175 0.145 0.364 0.991 0.360 0.688 0.793 0.567 0.429 0.707 0.435 0.940 0.428 0.549 0.953 0.985 0.399 0.530 0.267 0.488 0.303 0.007 0.877 0.909 0.291 0.986 0.534 0.797 0.650 0.397 0.859 0.116 0.199 0.114 0.942 0.172 0.495 0.980 0.820 0.064 0.755 0.599 0.266 0.571 0.014 0.424 0.259 0.185 0.665 0.700 0.059 0.532 0.197 0.174 0.868 0.815 0.644 0.070 0.608 0.993 0.983]
                  [0.748 0.530 0.342 0.481 0.604 0.946 0.734 0.768 0.249 0.616 0.424 0.265 0.737 0.993 0.401 0.405 0.611 0.795 0.382 0.008 0.854 0.139 0.346 0.072 0.395 0.779 0.102 0.170 0.707 0.827 0.748 0.653 0.131 0.707 0.059 0.848 0.024 0.976 0.747 0.482 0.525 0.110 0.298 0.631 0.690 0.278 0.547 0.615 0.053 0.638 0.474 0.366 0.191 0.718 0.734 0.978 0.715 0.995 0.305 0.349 0.205 0.857 0.256 0.033 0.012 0.785 0.022 0.695 0.520 0.710 0.812 0.850 0.404 0.191 0.680 0.879 0.674 0.282 0.606 0.686]
                  [0.796 0.687 0.411 0.716 0.985 0.073 0.688 0.491 0.845 0.236 0.983 0.277 0.887 0.128 0.375 0.923 0.106 0.642 0.245 0.496 0.712 0.436 0.443 0.795 0.314 0.017 0.326 0.472 0.353 0.760 0.095 0.045 0.334 0.546 0.670 0.719 0.452 0.199 0.903 0.643 0.565 0.993 0.253 0.866 0.025 0.056 0.207 0.030 0.831 0.636 0.598 0.571 0.688 0.833 0.700 0.997 0.046 0.583 0.416 0.640 0.901 0.407 0.153 0.113 0.189 0.645 0.982 0.610 0.865 0.716 0.526 0.088 0.369 0.216 0.586 0.304 0.016 0.512 0.914 0.705]])
        
        (def ww2 [[0.903 0.211 0.309 0.679 0.816 0.638 0.404 0.421 0.124 0.288 0.685 0.759 0.252 0.042 0.930 0.046 0.767 0.925 0.218 0.902 0.614 0.645 0.581 0.052 0.686 0.803 0.893 0.863 0.048 0.634 0.546 0.138 0.036 0.276 0.360 0.479 0.613 0.197 0.872 0.312 0.829 0.616 0.620 0.498 0.953 0.423 0.270 0.310 0.877 0.400 0.502 0.426 0.692 0.269 0.347 0.509 0.879 0.445 0.281 0.910 0.482 0.157 0.761 0.963 0.704 0.024 0.034 0.407 0.844 0.025 0.996 0.452 0.156 0.267 0.591 0.136 0.231 0.891 0.753 0.659]
                  [0.740 0.969 0.160 0.579 0.953 0.254 0.153 0.862 0.236 0.219 0.989 0.022 0.898 0.884 0.257 0.772 0.093 0.568 0.662 0.904 0.187 0.843 0.596 0.384 0.955 0.653 0.865 0.962 0.188 0.770 0.702 0.986 0.827 0.490 0.733 0.440 0.536 0.782 0.855 0.693 0.682 0.858 0.115 0.857 0.090 0.523 0.518 0.140 0.545 0.480 0.368 0.096 0.961 0.208 0.194 0.208 0.328 0.936 0.507 0.803 0.041 0.704 0.657 0.921 0.967 0.500 0.831 0.371 0.686 0.981 0.799 0.887 0.626 0.343 0.359 0.993 0.462 0.492 0.559 0.905]
                  [0.022 0.791 0.678 0.555 0.115 0.297 0.648 0.635 0.151 0.588 0.046 0.352 0.177 0.003 0.061 0.443 0.935 0.338 0.857 0.909 0.871 0.833 0.577 0.804 0.604 0.591 0.809 0.384 0.210 0.391 0.742 0.900 0.821 0.244 0.368 0.318 0.476 0.935 0.036 0.284 0.847 0.207 0.391 0.749 0.460 0.237 0.215 0.737 0.286 0.524 0.989 0.911 0.149 0.330 0.037 0.706 0.185 0.397 0.985 0.061 0.753 0.721 0.855 0.942 0.529 0.793 0.570 0.157 0.240 0.882 0.791 0.227 0.415 0.579 0.996 0.408 0.782 0.081 0.472 0.309]
                  [0.446 0.383 0.400 0.939 0.789 0.879 0.541 0.427 0.116 0.618 0.126 0.092 0.413 0.854 0.425 0.063 0.241 0.354 0.123 0.090 0.715 0.360 0.081 0.789 0.702 0.993 0.271 0.098 0.878 0.408 0.679 0.339 0.043 0.925 0.743 0.910 0.137 0.966 0.473 0.294 0.722 0.614 0.734 0.780 0.345 0.252 0.297 0.998 0.020 0.809 0.175 0.150 0.316 0.840 0.447 0.595 0.924 0.804 0.255 0.271 0.369 0.637 0.881 0.300 0.502 0.964 0.875 0.978 0.735 0.054 0.000 0.304 0.558 0.937 0.360 0.120 0.719 0.938 0.450 0.920]
                  [0.607 0.251 0.035 0.403 0.804 0.072 0.684 0.868 0.448 0.243 0.522 0.642 0.081 0.325 0.260 0.812 0.873 0.501 0.836 0.685 0.690 0.749 0.081 0.544 0.306 0.266 0.644 0.502 0.151 0.214 0.068 0.425 0.387 0.493 0.279 0.211 0.783 0.368 0.977 0.977 0.808 0.487 0.353 0.016 0.591 0.012 0.335 0.756 0.925 0.344 0.074 0.345 0.289 0.203 0.722 0.866 0.923 0.602 0.611 0.669 0.381 0.634 0.867 0.767 0.595 0.811 0.099 0.753 0.444 0.131 0.282 0.237 0.988 0.967 0.147 0.369 0.035 0.318 0.407 0.631]
                  [0.479 0.991 0.925 0.693 0.871 0.768 0.487 0.807 0.046 0.698 0.201 0.913 0.206 0.218 0.659 0.344 0.784 0.950 0.907 0.789 0.553 0.644 0.963 0.951 0.937 0.934 0.088 0.178 0.166 0.284 0.258 0.168 0.996 0.748 0.340 0.768 0.944 0.912 0.260 0.763 0.635 0.755 0.070 0.537 0.663 0.928 0.179 0.435 0.534 0.376 0.186 0.677 0.700 0.095 0.673 0.715 0.351 0.271 0.059 0.997 0.659 0.972 0.093 0.188 0.414 0.110 0.861 0.063 0.551 0.367 0.248 0.415 0.878 0.357 0.621 0.557 0.452 0.384 0.154 0.181]
                  [0.413 0.770 0.595 0.638 0.788 0.864 0.489 0.961 0.696 0.559 0.808 0.290 0.664 0.302 0.739 0.713 0.237 0.387 0.852 0.909 0.891 0.542 0.244 0.396 0.334 0.658 0.300 0.651 0.624 0.834 0.622 0.824 0.192 0.786 0.322 0.278 0.778 0.054 0.296 0.755 0.478 0.388 0.364 0.505 0.615 0.155 0.092 0.326 0.866 0.577 0.973 0.123 0.186 0.178 0.134 0.218 0.340 0.824 0.132 0.373 0.510 0.376 0.510 0.191 0.303 0.051 0.681 0.346 0.681 0.270 0.422 0.849 0.773 0.921 0.696 0.409 0.283 0.712 0.810 0.781]
                  [0.193 0.496 0.909 0.896 0.798 0.256 0.656 0.445 0.653 0.720 0.515 0.595 0.899 0.423 0.446 0.430 0.555 0.412 0.971 0.760 0.804 0.658 0.517 0.974 0.999 0.709 0.639 0.662 0.084 0.051 0.264 0.867 0.433 0.149 0.348 0.936 0.600 0.154 0.409 0.114 0.356 0.847 0.256 0.224 0.472 0.430 0.838 0.995 0.092 0.440 0.577 0.867 0.241 0.180 0.694 0.390 0.937 0.751 0.047 0.650 0.633 0.993 0.728 0.511 0.214 0.361 0.207 0.858 0.592 0.054 0.113 0.407 0.003 0.534 0.863 0.144 0.479 0.857 0.193 0.880]
                  [0.607 0.182 0.318 0.960 0.613 0.152 0.905 0.684 0.013 0.812 0.216 0.560 0.550 0.194 0.833 0.147 0.910 0.649 0.111 0.828 0.769 0.600 0.113 0.578 0.019 0.371 0.087 0.429 0.674 0.390 0.685 0.107 0.805 0.158 0.056 0.050 0.069 0.022 0.582 0.818 0.817 0.984 0.549 0.715 0.515 0.489 0.255 0.318 0.232 0.351 0.120 0.453 0.240 0.803 0.147 0.121 0.352 0.903 0.708 0.793 0.492 0.416 0.984 0.460 0.112 0.617 0.545 0.876 0.522 0.405 0.516 0.425 0.878 0.499 0.464 0.147 0.578 0.492 0.669 0.545]
                  [0.065 0.001 0.707 0.922 0.596 0.202 0.343 0.197 0.369 0.567 0.578 0.295 0.482 0.734 0.316 0.470 0.640 0.032 0.533 0.175 0.145 0.364 0.991 0.360 0.688 0.793 0.567 0.429 0.707 0.435 0.940 0.428 0.549 0.953 0.985 0.399 0.530 0.267 0.488 0.303 0.007 0.877 0.909 0.291 0.986 0.534 0.797 0.650 0.397 0.859 0.116 0.199 0.114 0.942 0.172 0.495 0.980 0.820 0.064 0.755 0.599 0.266 0.571 0.014 0.424 0.259 0.185 0.665 0.700 0.059 0.532 0.197 0.174 0.868 0.815 0.644 0.070 0.608 0.993 0.983]
                  [0.748 0.530 0.342 0.481 0.604 0.946 0.734 0.768 0.249 0.616 0.424 0.265 0.737 0.993 0.401 0.405 0.611 0.795 0.382 0.008 0.854 0.139 0.346 0.072 0.395 0.779 0.102 0.170 0.707 0.827 0.748 0.653 0.131 0.707 0.059 0.848 0.024 0.976 0.747 0.482 0.525 0.110 0.298 0.631 0.690 0.278 0.547 0.615 0.053 0.638 0.474 0.366 0.191 0.718 0.734 0.978 0.715 0.995 0.305 0.349 0.205 0.857 0.256 0.033 0.012 0.785 0.022 0.695 0.520 0.710 0.812 0.850 0.404 0.191 0.680 0.879 0.674 0.282 0.606 0.686]
                  [0.796 0.687 0.411 0.716 0.985 0.073 0.688 0.491 0.845 0.236 0.983 0.277 0.887 0.128 0.375 0.923 0.106 0.642 0.245 0.496 0.712 0.436 0.443 0.795 0.314 0.017 0.326 0.472 0.353 0.760 0.095 0.045 0.334 0.546 0.670 0.719 0.452 0.199 0.903 0.643 0.565 0.993 0.253 0.866 0.025 0.056 0.207 0.030 0.831 0.636 0.598 0.571 0.688 0.833 0.700 0.997 0.046 0.583 0.416 0.640 0.901 0.407 0.153 0.113 0.189 0.645 0.982 0.610 0.865 0.716 0.526 0.088 0.369 0.216 0.586 0.304 0.016 0.512 0.914 0.705]])
        
        ; (feed-one wx [ww1 ww2] 0.1 :training false)

        (println "\nError -" (first ec))
        (println "Err % -" (* 100.0 ymean))
          
        (when verbose
          (println " [y        yhat    |y-yhat| ]")
          (pmm (last ec)))))

(defn -main
  "ANN to predict hotness of a song, sgd optimization"
  [& args])
